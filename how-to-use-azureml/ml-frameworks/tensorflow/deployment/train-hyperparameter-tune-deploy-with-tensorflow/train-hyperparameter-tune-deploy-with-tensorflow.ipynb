{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/training-with-deep-learning/train-hyperparameter-tune-deploy-with-tensorflow/train-hyperparameter-tune-deploy-with-tensorflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bf74d2e9-2708-49b1-934b-e0ede342f475"
    }
   },
   "source": [
    "# Training, hyperparameter tune, and deploy with TensorFlow\n",
    "\n",
    "## Introduction\n",
    "This tutorial shows how to train a simple deep neural network using the MNIST dataset and TensorFlow on Azure Machine Learning. MNIST is a popular dataset consisting of 70,000 grayscale images. Each image is a handwritten digit of `28x28` pixels, representing number from 0 to 9. The goal is to create a multi-class classifier to identify the digit each image represents, and deploy it as a web service in Azure.\n",
    "\n",
    "For more information about the MNIST dataset, please visit [Yan LeCun's website](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started. First let's import some Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "c377ea0c-0cd9-4345-9be2-e20fb29c94c3"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "edaa7f2f-2439-4148-b57a-8c794c0945ec"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.7.0\n"
     ]
    }
   ],
   "source": [
    "import azureml\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning diagnostics collection on. \n"
     ]
    }
   ],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: wu2modtimesmlsw\n",
      "Azure region: westus2\n",
      "Subscription id: 7fd76d0f-84f2-498b-a997-e0d059af5ce1\n",
      "Resource group: wu2modtimerg\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "59f52294-4a25-4c92-bab8-3b07f0f44d15"
    }
   },
   "source": [
    "## Create an Azure ML experiment\n",
    "Let's create an experiment named \"tf-mnist\" and a folder to hold the training scripts. The script runs will be recorded under the experiment in Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "bc70f780-c240-4779-96f3-bc5ef9a37d59"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "script_folder = './tf-mnist'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "exp = Experiment(workspace=ws, name='tf-mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "defe921f-8097-44c3-8336-8af6700804a7"
    }
   },
   "source": [
    "## Download MNIST dataset\n",
    "In order to train on the MNIST dataset we will first need to download it from azuremlopendatasets blob directly and save them in a `data` folder locally. If you want you can directly download the same data from Yan LeCun's web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\repos\\\\git\\\\MachineLearningNotebooks\\\\how-to-use-azureml\\\\ml-frameworks\\\\tensorflow\\\\deployment\\\\train-hyperparameter-tune-deploy-with-tensorflow\\\\data\\\\t10k-labels-idx1-ubyte.gz',\n",
       " <http.client.HTTPMessage at 0x210511b1908>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 'train-images-idx3-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'))\n",
    "urllib.request.urlretrieve('https://azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz',\n",
    "                           filename=os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c3f2f57c-7454-4d3e-b38d-b0946cf066ea"
    }
   },
   "source": [
    "## Show some sample images\n",
    "Let's load the downloaded compressed file into numpy arrays using some utility functions included in the `utils.py` library file from the current folder. Then we use `matplotlib` to plot 30 random images from the dataset along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "396d478b-34aa-4afa-9898-cdce8222a516"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import load_data\n",
    "\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the neural network converge faster.\n",
    "X_train = load_data(os.path.join(data_folder, 'train-images-idx3-ubyte.gz'), False) / np.float32(255.0)\n",
    "X_test = load_data(os.path.join(data_folder, 't10k-images-idx3-ubyte.gz'), False) / np.float32(255.0)\n",
    "y_train = load_data(os.path.join(data_folder, 'train-labels-idx1-ubyte.gz'), True).reshape(-1)\n",
    "y_test = load_data(os.path.join(data_folder, 't10k-labels-idx1-ubyte.gz'), True).reshape(-1)\n",
    "\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x = 10, y = -10, s = y_train[i], fontsize = 18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap = plt.cm.Greys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset for Files\n",
    "A Dataset can reference single or multiple files in your datastores or public urls. The files can be of any format. Dataset provides you with the ability to download or mount the files to your compute. By creating a dataset, you create a reference to the data source location. If you applied any subsetting transformations to the dataset, they will be stored in the dataset as well. The data remains in its existing location, so no extra storage cost is incurred. [Learn More](https://aka.ms/azureml/howto/createdatasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "web_paths = ['https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "dataset = Dataset.File.from_files(path = web_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to regiester datasets using the register() method to your workspace so they can be shared with others, reused across various experiments, and referred to by name in your training script.\n",
    "You can try get the dataset first to see if it's already registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset mnist-dataset is not registered in workspace yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/https/azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
       " '/https/azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz',\n",
       " '/https/azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz',\n",
       " '/https/azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_registered = False\n",
    "try:\n",
    "    temp = Dataset.get_by_name(workspace = ws, name = 'mnist-dataset')\n",
    "    dataset_registered = True\n",
    "except:\n",
    "    print(\"The dataset mnist-dataset is not registered in workspace yet.\")\n",
    "\n",
    "if not dataset_registered:\n",
    "    dataset = dataset.register(workspace = ws,\n",
    "                               name = 'mnist-dataset',\n",
    "                               description='training and test dataset',\n",
    "                               create_new_version=True)\n",
    "# list the files referenced by dataset\n",
    "dataset.to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach existing AmlCompute\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training your model. In this tutorial, you create `AmlCompute` as your training compute resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we could not find the cluster with the given name, then we will create a new cluster here. We will create an `AmlCompute` cluster of `STANDARD_NC6` GPU VMs. This process is broken down into 3 steps:\n",
    "1. create the configuration (this step is local and only takes a second)\n",
    "2. create the cluster (this step will take about **20 seconds**)\n",
    "3. provision the VMs to bring the cluster to the initial size (of 1 in this case). This step will take about **3-5 minutes** and is providing only sparse output in the process. Please make sure to wait until the call returns before moving to the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-06-25T22:57:32.180000+00:00', 'errors': None, 'creationTime': '2020-01-15T19:21:21.333672+00:00', 'modifiedTime': '2020-01-15T19:21:36.994850+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 6, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have created the compute target, let's see what the workspace's `compute_targets` property returns. You should now see one entry named 'gpu-cluster' of type `AmlCompute`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan-classifier VirtualMachine Succeeded\n",
      "pfg VirtualMachine Succeeded\n",
      "cpucluster AmlCompute Succeeded\n",
      "gpucluster AmlCompute Succeeded\n",
      "cpu-cluster AmlCompute Succeeded\n",
      "gpu-cluster AmlCompute Succeeded\n",
      "gpunotebooks ComputeInstance Succeeded\n",
      "githubcluster AmlCompute Succeeded\n"
     ]
    }
   ],
   "source": [
    "compute_targets = ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the training files into the script folder\n",
    "The TensorFlow training script is already created for you. You can simply copy it into the script folder, together with the utility library used to load compressed data file into numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./tf-mnist\\\\utils.py'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# the training logic is in the tf_mnist.py file.\n",
    "shutil.copy('./tf_mnist.py', script_folder)\n",
    "\n",
    "# the utils.py just helps loading data from the downloaded MNIST dataset into numpy arrays.\n",
    "shutil.copy('./utils.py', script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2039d2d5-aca6-4f25-a12f-df9ae6529cae"
    }
   },
   "source": [
    "## Construct neural network in TensorFlow\n",
    "In the training script `tf_mnist.py`, it creates a very simple DNN (deep neural network), with just 2 hidden layers. The input layer has 28 * 28 = 784 neurons, each representing a pixel in an image. The first hidden layer has 300 neurons, and the second hidden layer has 100 neurons. The output layer has 10 neurons, each representing a targeted label from 0 to 9.\n",
    "\n",
    "![DNN](nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure ML concepts  \n",
    "Please note the following three things in the code below:\n",
    "1. The script accepts arguments using the argparse package. In this case there is one argument `--data_folder` which specifies the file system folder in which the script can find the MNIST data\n",
    "```\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_folder')\n",
    "```\n",
    "2. The script is accessing the Azure ML `Run` object by executing `run = Run.get_context()`. Further down the script is using the `run` to report the training accuracy and the validation accuracy as training progresses.\n",
    "```\n",
    "    run.log('training_acc', np.float(acc_train))\n",
    "    run.log('validation_acc', np.float(acc_val))\n",
    "```\n",
    "3. When running the script on Azure ML, you can write files out to a folder `./outputs` that is relative to the root directory. This folder is specially tracked by Azure ML in the sense that any files written to that folder during script execution on the remote target will be picked up by Run History; these files (known as artifacts) will be available as part of the run history record."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will print out the training code for you to inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
      "# Licensed under the MIT License.\n",
      "\n",
      "import numpy as np\n",
      "import argparse\n",
      "import os\n",
      "import re\n",
      "import tensorflow as tf\n",
      "import time\n",
      "import glob\n",
      "\n",
      "from azureml.core import Run\n",
      "from utils import load_data\n",
      "from tensorflow.keras import Model, layers\n",
      "\n",
      "\n",
      "# Create TF Model.\n",
      "class NeuralNet(Model):\n",
      "    # Set layers.\n",
      "    def __init__(self):\n",
      "        super(NeuralNet, self).__init__()\n",
      "        # First hidden layer.\n",
      "        self.h1 = layers.Dense(n_h1, activation=tf.nn.relu)\n",
      "        # Second hidden layer.\n",
      "        self.h2 = layers.Dense(n_h2, activation=tf.nn.relu)\n",
      "        self.out = layers.Dense(n_outputs)\n",
      "\n",
      "    # Set forward pass.\n",
      "    def call(self, x, is_training=False):\n",
      "        x = self.h1(x)\n",
      "        x = self.h2(x)\n",
      "        x = self.out(x)\n",
      "        if not is_training:\n",
      "            # Apply softmax when not training.\n",
      "            x = tf.nn.softmax(x)\n",
      "        return x\n",
      "\n",
      "\n",
      "def cross_entropy_loss(y, logits):\n",
      "    # Convert labels to int 64 for tf cross-entropy function.\n",
      "    y = tf.cast(y, tf.int64)\n",
      "    # Apply softmax to logits and compute cross-entropy.\n",
      "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
      "    # Average loss across the batch.\n",
      "    return tf.reduce_mean(loss)\n",
      "\n",
      "\n",
      "# Accuracy metric.\n",
      "def accuracy(y_pred, y_true):\n",
      "    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
      "    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
      "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n",
      "\n",
      "\n",
      "# Optimization process.\n",
      "def run_optimization(x, y):\n",
      "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
      "    with tf.GradientTape() as g:\n",
      "        # Forward pass.\n",
      "        logits = neural_net(x, is_training=True)\n",
      "        # Compute loss.\n",
      "        loss = cross_entropy_loss(y, logits)\n",
      "\n",
      "    # Variables to update, i.e. trainable variables.\n",
      "    trainable_variables = neural_net.trainable_variables\n",
      "\n",
      "    # Compute gradients.\n",
      "    gradients = g.gradient(loss, trainable_variables)\n",
      "\n",
      "    # Update W and b following gradients.\n",
      "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
      "\n",
      "\n",
      "print(\"TensorFlow version:\", tf.__version__)\n",
      "\n",
      "parser = argparse.ArgumentParser()\n",
      "parser.add_argument('--data-folder', type=str, dest='data_folder', default='data', help='data folder mounting point')\n",
      "parser.add_argument('--batch-size', type=int, dest='batch_size', default=128, help='mini batch size for training')\n",
      "parser.add_argument('--first-layer-neurons', type=int, dest='n_hidden_1', default=128,\n",
      "                    help='# of neurons in the first layer')\n",
      "parser.add_argument('--second-layer-neurons', type=int, dest='n_hidden_2', default=128,\n",
      "                    help='# of neurons in the second layer')\n",
      "parser.add_argument('--learning-rate', type=float, dest='learning_rate', default=0.01, help='learning rate')\n",
      "parser.add_argument('--resume-from', type=str, default=None,\n",
      "                    help='location of the model or checkpoint files from where to resume the training')\n",
      "args = parser.parse_args()\n",
      "\n",
      "previous_model_location = args.resume_from\n",
      "# You can also use environment variable to get the model/checkpoint files location\n",
      "# previous_model_location = os.path.expandvars(os.getenv(\"AZUREML_DATAREFERENCE_MODEL_LOCATION\", None))\n",
      "\n",
      "data_folder = args.data_folder\n",
      "print('Data folder:', data_folder)\n",
      "\n",
      "# load train and test set into numpy arrays\n",
      "# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\n",
      "X_train = load_data(glob.glob(os.path.join(data_folder, '**/train-images-idx3-ubyte.gz'),\n",
      "                              recursive=True)[0], False) / np.float32(255.0)\n",
      "X_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-images-idx3-ubyte.gz'),\n",
      "                             recursive=True)[0], False) / np.float32(255.0)\n",
      "y_train = load_data(glob.glob(os.path.join(data_folder, '**/train-labels-idx1-ubyte.gz'),\n",
      "                              recursive=True)[0], True).reshape(-1)\n",
      "y_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-labels-idx1-ubyte.gz'),\n",
      "                             recursive=True)[0], True).reshape(-1)\n",
      "\n",
      "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep='\\n')\n",
      "\n",
      "training_set_size = X_train.shape[0]\n",
      "\n",
      "n_inputs = 28 * 28\n",
      "n_h1 = args.n_hidden_1\n",
      "n_h2 = args.n_hidden_2\n",
      "n_outputs = 10\n",
      "learning_rate = args.learning_rate\n",
      "n_epochs = 20\n",
      "batch_size = args.batch_size\n",
      "\n",
      "# Build neural network model.\n",
      "neural_net = NeuralNet()\n",
      "\n",
      "# Stochastic gradient descent optimizer.\n",
      "optimizer = tf.optimizers.SGD(learning_rate)\n",
      "\n",
      "# start an Azure ML run\n",
      "run = Run.get_context()\n",
      "\n",
      "if previous_model_location:\n",
      "    # Restore variables from latest checkpoint.\n",
      "    checkpoint = tf.train.Checkpoint(model=neural_net, optimizer=optimizer)\n",
      "    checkpoint_file_path = tf.train.latest_checkpoint(previous_model_location)\n",
      "    checkpoint.restore(checkpoint_file_path)\n",
      "    checkpoint_filename = os.path.basename(checkpoint_file_path)\n",
      "    num_found = re.search(r'\\d+', checkpoint_filename)\n",
      "    if num_found:\n",
      "        start_epoch = int(num_found.group(0))\n",
      "        print(\"Resuming from epoch {}\".format(str(start_epoch)))\n",
      "\n",
      "start_time = time.perf_counter()\n",
      "for epoch in range(0, n_epochs):\n",
      "\n",
      "    # randomly shuffle training set\n",
      "    indices = np.random.permutation(training_set_size)\n",
      "    X_train = X_train[indices]\n",
      "    y_train = y_train[indices]\n",
      "\n",
      "    # batch index\n",
      "    b_start = 0\n",
      "    b_end = b_start + batch_size\n",
      "    for _ in range(training_set_size // batch_size):\n",
      "        # get a batch\n",
      "        X_batch, y_batch = X_train[b_start: b_end], y_train[b_start: b_end]\n",
      "\n",
      "        # update batch index for the next batch\n",
      "        b_start = b_start + batch_size\n",
      "        b_end = min(b_start + batch_size, training_set_size)\n",
      "\n",
      "        # train\n",
      "        run_optimization(X_batch, y_batch)\n",
      "\n",
      "    # evaluate training set\n",
      "    pred = neural_net(X_batch, is_training=False)\n",
      "    acc_train = accuracy(pred, y_batch)\n",
      "\n",
      "    # evaluate validation set\n",
      "    pred = neural_net(X_test, is_training=False)\n",
      "    acc_val = accuracy(pred, y_test)\n",
      "\n",
      "    # log accuracies\n",
      "    run.log('training_acc', np.float(acc_train))\n",
      "    run.log('validation_acc', np.float(acc_val))\n",
      "    print(epoch, '-- Training accuracy:', acc_train, '\\b Validation accuracy:', acc_val)\n",
      "\n",
      "    # Save checkpoints in the \"./outputs\" folder so that they are automatically uploaded into run history.\n",
      "    checkpoint_dir = './outputs/'\n",
      "    checkpoint = tf.train.Checkpoint(model=neural_net, optimizer=optimizer)\n",
      "\n",
      "    if epoch % 2 == 0:\n",
      "        checkpoint.save(checkpoint_dir)\n",
      "\n",
      "run.log('final_acc', np.float(acc_val))\n",
      "os.makedirs('./outputs/model', exist_ok=True)\n",
      "\n",
      "# files saved in the \"./outputs\" folder are automatically uploaded into run history\n",
      "# this is workaround for https://github.com/tensorflow/tensorflow/issues/33913 and will be fixed once we move to >tf2.1\n",
      "neural_net._set_inputs(X_train)\n",
      "tf.saved_model.save(neural_net, './outputs/model/')\n",
      "\n",
      "stop_time = time.perf_counter()\n",
      "training_time = (stop_time - start_time) * 1000\n",
      "print(\"Total time in milliseconds for training: {}\".format(str(training_time)))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(script_folder, './tf_mnist.py'), 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TensorFlow estimator\n",
    "Next, we construct an `azureml.train.dnn.TensorFlow` estimator object, use the Batch AI cluster as compute target, and pass the mount-point of the datastore to the training code as a parameter.\n",
    "\n",
    "The TensorFlow estimator is providing a simple way of launching a TensorFlow training job on a compute target. It will automatically provide a docker image that has TensorFlow installed -- if additional pip or conda packages are required, their names can be passed in via the `pip_packages` and `conda_packages` arguments and they will be included in the resulting docker.\n",
    "\n",
    "The TensorFlow estimator also takes a `framework_version` parameter -- if no version is provided, the estimator will default to the latest version supported by AzureML. Use `TensorFlow.get_supported_versions()` to get a list of all versions supported by your current SDK version or see the [SDK documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn?view=azure-ml-py) for the versions supported in the most current release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "dnn-tensorflow-remarks-sample"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['tensorflow-gpu']. We cannot guarantee image build will succeed.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': dataset.as_named_input('mnist').as_mount(),\n",
    "    '--batch-size': 64,\n",
    "    '--first-layer-neurons': 256,\n",
    "    '--second-layer-neurons': 128,\n",
    "    '--learning-rate': 0.01\n",
    "}\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='tf_mnist.py',\n",
    "                 use_gpu=True,\n",
    "                 framework_version='2.1',\n",
    "                 pip_packages=['azureml-dataprep[pandas,fuse]','tensorflow-gpu==2.2.0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job to run\n",
    "Submit the estimator to an Azure ML experiment to kick off the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the Run <a class=\"anchor\" id=\"monitor-run\"></a>\n",
    "As the Run is executed, it will go through the following stages:\n",
    "1. Preparing: A docker image is created matching the Python environment specified by the TensorFlow estimator and it will be uploaded to the workspace's Azure Container Registry. This step will only happen once for each Python environment -- the container will then be cached for subsequent runs. Creating and uploading the image takes about **5 minutes**. While the job is preparing, logs are streamed to the run history and can be viewed to monitor the progress of the image creation.\n",
    "\n",
    "2. Scaling: If the compute needs to be scaled up (i.e. the Batch AI cluster requires more nodes to execute the run than currently available), the cluster will attempt to scale up in order to make the required amount of nodes available. Scaling typically takes about **5 minutes**.\n",
    "\n",
    "3. Running: All scripts in the script folder are uploaded to the compute target, data stores are mounted/copied and the `entry_script` is executed. While the job is running, stdout and the `./logs` folder are streamed to the run history and can be viewed to monitor the progress of the run.\n",
    "\n",
    "4. Post-Processing: The `./outputs` folder of the run is copied over to the run history\n",
    "\n",
    "There are multiple ways to check the progress of a running job. We can use a Jupyter notebook widget. \n",
    "\n",
    "**Note: The widget will automatically update ever 10-15 seconds, always showing you the most up-to-date information about the run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2282289f4e2041a6b1010ba29d39e74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Preparing\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/tf-mnist/runs/tf-mnist_1593127650_217d3343?wsid=/subscriptions/7fd76d0f-84f2-498b-a997-e0d059af5ce1/resourcegroups/wu2modtimerg/workspaces/wu2modtimesmlsw\", \"run_id\": \"tf-mnist_1593127650_217d3343\", \"run_properties\": {\"run_id\": \"tf-mnist_1593127650_217d3343\", \"created_utc\": \"2020-06-25T23:27:34.618231Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"28da1ffc-b78c-40e4-bb86-820d8b30730e\", \"azureml.git.repository_uri\": \"https://github.com/Azure/MachineLearningNotebooks.git\", \"mlflow.source.git.repoURL\": \"https://github.com/Azure/MachineLearningNotebooks.git\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"053efde8c9740c7b691c4d13ee1f5b5b206cd24f\", \"mlflow.source.git.commit\": \"053efde8c9740c7b691c4d13ee1f5b5b206cd24f\", \"azureml.git.dirty\": \"True\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Preparing\", \"log_files\": {\"azureml-logs/20_image_build_log.txt\": \"https://wu2modtimesmls2858026735.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1593127650_217d3343/azureml-logs/20_image_build_log.txt?sv=2019-02-02&sr=b&sig=FKTapzXZjquN0kJs9tJQyDz62FYGaWqfvBAYdeXso1A%3D&st=2020-06-25T23%3A22%3A46Z&se=2020-06-26T07%3A32%3A46Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/20_image_build_log.txt\"]], \"run_duration\": \"0:05:10\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"2020/06/25 23:27:37 Downloading source code...\\r\\n2020/06/25 23:27:38 Finished downloading source code\\r\\n2020/06/25 23:27:39 Creating Docker network: acb_default_network, driver: 'bridge'\\n2020/06/25 23:27:39 Successfully set up Docker network: acb_default_network\\n2020/06/25 23:27:39 Setting up Docker configuration...\\n2020/06/25 23:27:40 Successfully set up Docker configuration\\n2020/06/25 23:27:40 Logging in to registry: wu2modtimesmls5288786732.azurecr.io\\n2020/06/25 23:27:40 Successfully logged into wu2modtimesmls5288786732.azurecr.io\\n2020/06/25 23:27:40 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\\n2020/06/25 23:27:40 Scanning for dependencies...\\r\\n2020/06/25 23:27:41 Successfully scanned dependencies\\n2020/06/25 23:27:41 Launching container with name: acb_step_0\\nSending build context to Docker daemon  60.93kB\\r\\r\\nStep 1/14 : FROM mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04:20200423.v1@sha256:b403215d04671379ed5e413119f45411e444e85369ac7965c6b720db83f1f951\\nsha256:b403215d04671379ed5e413119f45411e444e85369ac7965c6b720db83f1f951: Pulling from azureml/openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04\\n7ddbc47eeb70: Pulling fs layer\\nc1bbdc448b72: Pulling fs layer\\n8c3b70e39044: Pulling fs layer\\n45d437916d57: Pulling fs layer\\nd8f1569ddae6: Pulling fs layer\\nde5a2c57c41d: Pulling fs layer\\nea6f04a00543: Pulling fs layer\\n7b872974e97c: Pulling fs layer\\nba8a751eb7d9: Pulling fs layer\\nc23d51d23979: Pulling fs layer\\n9757d27dcd8d: Pulling fs layer\\ncabeffbcfc90: Pulling fs layer\\nda0b272d5cf7: Pulling fs layer\\n8f96f704f99a: Pulling fs layer\\nde9dcc8b8be7: Pulling fs layer\\n34d20e2bbbbc: Pulling fs layer\\nce7448095986: Pulling fs layer\\n45d437916d57: Waiting\\nd8f1569ddae6: Waiting\\nde5a2c57c41d: Waiting\\nea6f04a00543: Waiting\\n7b872974e97c: Waiting\\nba8a751eb7d9: Waiting\\nc23d51d23979: Waiting\\n9757d27dcd8d: Waiting\\ncabeffbcfc90: Waiting\\nda0b272d5cf7: Waiting\\n8f96f704f99a: Waiting\\nde9dcc8b8be7: Waiting\\n34d20e2bbbbc: Waiting\\nce7448095986: Waiting\\nc1bbdc448b72: Verifying Checksum\\nc1bbdc448b72: Download complete\\n8c3b70e39044: Verifying Checksum\\n8c3b70e39044: Download complete\\n45d437916d57: Verifying Checksum\\n45d437916d57: Download complete\\nd8f1569ddae6: Verifying Checksum\\nd8f1569ddae6: Download complete\\r\\n7ddbc47eeb70: Download complete\\nea6f04a00543: Verifying Checksum\\nea6f04a00543: Download complete\\nde5a2c57c41d: Verifying Checksum\\nde5a2c57c41d: Download complete\\n7b872974e97c: Verifying Checksum\\n7b872974e97c: Download complete\\r\\nc23d51d23979: Verifying Checksum\\nc23d51d23979: Download complete\\n9757d27dcd8d: Verifying Checksum\\n9757d27dcd8d: Download complete\\nda0b272d5cf7: Verifying Checksum\\nda0b272d5cf7: Download complete\\r\\ncabeffbcfc90: Verifying Checksum\\ncabeffbcfc90: Download complete\\nba8a751eb7d9: Verifying Checksum\\nba8a751eb7d9: Download complete\\n7ddbc47eeb70: Pull complete\\n34d20e2bbbbc: Verifying Checksum\\n34d20e2bbbbc: Download complete\\nce7448095986: Verifying Checksum\\nce7448095986: Download complete\\nde9dcc8b8be7: Verifying Checksum\\nde9dcc8b8be7: Download complete\\n8f96f704f99a: Download complete\\nc1bbdc448b72: Pull complete\\n8c3b70e39044: Pull complete\\n45d437916d57: Pull complete\\r\\nd8f1569ddae6: Pull complete\\nde5a2c57c41d: Pull complete\\nea6f04a00543: Pull complete\\n7b872974e97c: Pull complete\\r\\nba8a751eb7d9: Pull complete\\r\\nc23d51d23979: Pull complete\\r\\n9757d27dcd8d: Pull complete\\r\\ncabeffbcfc90: Pull complete\\nda0b272d5cf7: Pull complete\\n8f96f704f99a: Pull complete\\r\\nde9dcc8b8be7: Pull complete\\n34d20e2bbbbc: Pull complete\\nce7448095986: Pull complete\\nDigest: sha256:b403215d04671379ed5e413119f45411e444e85369ac7965c6b720db83f1f951\\nStatus: Downloaded newer image for mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04:20200423.v1@sha256:b403215d04671379ed5e413119f45411e444e85369ac7965c6b720db83f1f951\\n ---> 9dffe8ca9306\\nStep 2/14 : USER root\\n ---> Running in 89a8950b3778\\r\\nRemoving intermediate container 89a8950b3778\\n ---> c8b3185f23e5\\nStep 3/14 : RUN mkdir -p $HOME/.cache\\n ---> Running in 73324aee9f45\\nRemoving intermediate container 73324aee9f45\\n ---> 052744da525b\\nStep 4/14 : WORKDIR /\\r\\n ---> Running in 2c30c09f9d91\\nRemoving intermediate container 2c30c09f9d91\\n ---> ce6d10f1d5b3\\nStep 5/14 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\\n ---> 4f1661c53ab0\\nStep 6/14 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\\n ---> Running in 12aced4f19b8\\r\\nRemoving intermediate container 12aced4f19b8\\n ---> de4e602c8440\\nStep 7/14 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\\n ---> baec26682dd4\\nStep 8/14 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3 -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \\\"$HOME/.cache/pip\\\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \\\"$CONDA_ROOT_DIR/pkgs\\\" && find \\\"$CONDA_ROOT_DIR\\\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\\r\\n ---> Running in 5b0ea7eae76c\\nWarning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\\nCollecting package metadata (repodata.json): ...working... \\r\\ndone\\r\\nSolving environment: ...working... done\\n\\nDownloading and Extracting Packages\\n\\rsqlite-3.23.1        | 1.5 MB    |            |   0% \\rsqlite-3.23.1        | 1.5 MB    | ####3      |  43% \\rsqlite-3.23.1        | 1.5 MB    | ########## | 100% \\n\\rtk-8.6.10            | 3.2 MB    |            |   0% \\rtk-8.6.10            | 3.2 MB    | ########## | 100% \\n\\rreadline-7.0         | 387 KB    |            |   0% \\rreadline-7.0         | 387 KB    | ########## | 100% \\n\\rwheel-0.34.2         | 49 KB     |            |   0% \\rwheel-0.34.2         | 49 KB     | ########## | 100% \\n\\rlibffi-3.2.1         | 43 KB     |            |   0% \\rlibffi-3.2.1         | 43 KB     | ########## | 100% \\n\\rncurses-6.0          | 907 KB    |            |   0% \\rncurses-6.0          | 907 KB    | ########## | 100% \\r\\n\\rzlib-1.2.11          | 120 KB    |            |   0% \\rzlib-1.2.11          | 120 KB    | ########## | 100% \\n\\rlibedit-3.1          | 171 KB    |            |   0% \\rlibedit-3.1          | 171 KB    | ########## | 100% \\n\\ropenssl-1.0.2u       | 3.1 MB    |            |   0% \\ropenssl-1.0.2u       | 3.1 MB    | ########## | 100% \\n\\rcertifi-2020.6.20    | 160 KB    |            |   0% \\rcertifi-2020.6.20    | 160 KB    | ########## | 100% \\n\\rlibgcc-ng-9.1.0      | 8.1 MB    |            |   0% \\rlibgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \\n\\rlibstdcxx-ng-9.1.0   | 4.0 MB    |            |   0% \\rlibstdcxx-ng-9.1.0   | 4.0 MB    | ########## | 100% \\r\\n\\rca-certificates-2020 | 132 KB    |            |   0% \\rca-certificates-2020 | 132 KB    | ########## | 100% \\n\\rpython-3.6.2         | 27.0 MB   |            |   0% \\rpython-3.6.2         | 27.0 MB   | ###1       |  32% \\rpython-3.6.2         | 27.0 MB   | #######1   |  71% \\rpython-3.6.2         | 27.0 MB   | ########## | 100% \\r\\n\\rsetuptools-47.3.1    | 647 KB    |            |   0% \\rsetuptools-47.3.1    | 647 KB    | ########## | 100% \\n\\rxz-5.2.5             | 438 KB    |            |   0% \\rxz-5.2.5             | 438 KB    | ########## | 100% \\n\\rpip-20.1.1           | 2.0 MB    |            |   0% \\rpip-20.1.1           | 2.0 MB    | ########## | 100% \\nPreparing transaction: ...working... done\\nVerifying transaction: ...working... done\\nExecuting transaction: ...working... done\\r\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": true, \"log_level\": \"INFO\", \"sdk_version\": \"1.7.0\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also periodically check the status of the run object, and navigate to Azure portal to monitor the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>tf-mnist</td><td>tf-mnist_1593127650_217d3343</td><td>azureml.scriptrun</td><td>Preparing</td><td><a href=\"https://ml.azure.com/experiments/tf-mnist/runs/tf-mnist_1593127650_217d3343?wsid=/subscriptions/7fd76d0f-84f2-498b-a997-e0d059af5ce1/resourcegroups/wu2modtimerg/workspaces/wu2modtimesmlsw\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: tf-mnist,\n",
       "Id: tf-mnist_1593127650_217d3343,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Preparing)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: tf-mnist_1593127650_217d3343\n",
      "Web View: https://ml.azure.com/experiments/tf-mnist/runs/tf-mnist_1593127650_217d3343?wsid=/subscriptions/7fd76d0f-84f2-498b-a997-e0d059af5ce1/resourcegroups/wu2modtimerg/workspaces/wu2modtimesmlsw\n",
      "\n",
      "Streaming azureml-logs/20_image_build_log.txt\n",
      "=============================================\n",
      "\n",
      "2020/06/25 23:27:37 Downloading source code...\n",
      "2020/06/25 23:27:38 Finished downloading source code\n",
      "2020/06/25 23:27:39 Creating Docker network: acb_default_network, driver: 'bridge'\n",
      "2020/06/25 23:27:39 Successfully set up Docker network: acb_default_network\n",
      "2020/06/25 23:27:39 Setting up Docker configuration...\n",
      "2020/06/25 23:27:40 Successfully set up Docker configuration\n",
      "2020/06/25 23:27:40 Logging in to registry: wu2modtimesmls5288786732.azurecr.io\n",
      "2020/06/25 23:27:40 Successfully logged into wu2modtimesmls5288786732.azurecr.io\n",
      "2020/06/25 23:27:40 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2020/06/25 23:27:40 Scanning for dependencies...\n",
      "2020/06/25 23:27:41 Successfully scanned dependencies\n",
      "2020/06/25 23:27:41 Launching container with name: acb_step_0\n",
      "Sending build context to Docker daemon  60.93kB\n",
      "\n",
      "Step 1/14 : FROM mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04:20200423.v1@sha256:b403215d04671379ed5e413119f45411e444e85369ac7965c6b720db83f1f951\n",
      "sha256:b403215d04671379ed5e413119f45411e444e85369ac7965c6b720db83f1f951: Pulling from azureml/openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04\n",
      "7ddbc47eeb70: Pulling fs layer\n",
      "c1bbdc448b72: Pulling fs layer\n",
      "8c3b70e39044: Pulling fs layer\n",
      "45d437916d57: Pulling fs layer\n",
      "d8f1569ddae6: Pulling fs layer\n",
      "de5a2c57c41d: Pulling fs layer\n",
      "ea6f04a00543: Pulling fs layer\n",
      "7b872974e97c: Pulling fs layer\n",
      "ba8a751eb7d9: Pulling fs layer\n",
      "c23d51d23979: Pulling fs layer\n",
      "9757d27dcd8d: Pulling fs layer\n",
      "cabeffbcfc90: Pulling fs layer\n",
      "da0b272d5cf7: Pulling fs layer\n",
      "8f96f704f99a: Pulling fs layer\n",
      "de9dcc8b8be7: Pulling fs layer\n",
      "34d20e2bbbbc: Pulling fs layer\n",
      "ce7448095986: Pulling fs layer\n",
      "45d437916d57: Waiting\n",
      "d8f1569ddae6: Waiting\n",
      "de5a2c57c41d: Waiting\n",
      "ea6f04a00543: Waiting\n",
      "7b872974e97c: Waiting\n",
      "ba8a751eb7d9: Waiting\n",
      "c23d51d23979: Waiting\n",
      "9757d27dcd8d: Waiting\n",
      "cabeffbcfc90: Waiting\n",
      "da0b272d5cf7: Waiting\n",
      "8f96f704f99a: Waiting\n",
      "de9dcc8b8be7: Waiting\n",
      "34d20e2bbbbc: Waiting\n",
      "ce7448095986: Waiting\n",
      "c1bbdc448b72: Verifying Checksum\n",
      "c1bbdc448b72: Download complete\n",
      "8c3b70e39044: Verifying Checksum\n",
      "8c3b70e39044: Download complete\n",
      "45d437916d57: Verifying Checksum\n",
      "45d437916d57: Download complete\n",
      "d8f1569ddae6: Verifying Checksum\n",
      "d8f1569ddae6: Download complete\n",
      "7ddbc47eeb70: Download complete\n",
      "ea6f04a00543: Verifying Checksum\n",
      "ea6f04a00543: Download complete\n",
      "de5a2c57c41d: Verifying Checksum\n",
      "de5a2c57c41d: Download complete\n",
      "7b872974e97c: Verifying Checksum\n",
      "7b872974e97c: Download complete\n",
      "c23d51d23979: Verifying Checksum\n",
      "c23d51d23979: Download complete\n",
      "9757d27dcd8d: Verifying Checksum\n",
      "9757d27dcd8d: Download complete\n",
      "da0b272d5cf7: Verifying Checksum\n",
      "da0b272d5cf7: Download complete\n",
      "cabeffbcfc90: Verifying Checksum\n",
      "cabeffbcfc90: Download complete\n",
      "ba8a751eb7d9: Verifying Checksum\n",
      "ba8a751eb7d9: Download complete\n",
      "7ddbc47eeb70: Pull complete\n",
      "34d20e2bbbbc: Verifying Checksum\n",
      "34d20e2bbbbc: Download complete\n",
      "ce7448095986: Verifying Checksum\n",
      "ce7448095986: Download complete\n",
      "de9dcc8b8be7: Verifying Checksum\n",
      "de9dcc8b8be7: Download complete\n",
      "8f96f704f99a: Download complete\n",
      "c1bbdc448b72: Pull complete\n",
      "8c3b70e39044: Pull complete\n",
      "45d437916d57: Pull complete\n",
      "d8f1569ddae6: Pull complete\n",
      "de5a2c57c41d: Pull complete\n",
      "ea6f04a00543: Pull complete\n",
      "7b872974e97c: Pull complete\n",
      "ba8a751eb7d9: Pull complete\n",
      "c23d51d23979: Pull complete\n",
      "9757d27dcd8d: Pull complete\n",
      "cabeffbcfc90: Pull complete\n",
      "da0b272d5cf7: Pull complete\n",
      "8f96f704f99a: Pull complete\n",
      "de9dcc8b8be7: Pull complete\n",
      "34d20e2bbbbc: Pull complete\n",
      "ce7448095986: Pull complete\n",
      "Digest: sha256:b403215d04671379ed5e413119f45411e444e85369ac7965c6b720db83f1f951\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04:20200423.v1@sha256:b403215d04671379ed5e413119f45411e444e85369ac7965c6b720db83f1f951\n",
      " ---> 9dffe8ca9306\n",
      "Step 2/14 : USER root\n",
      " ---> Running in 89a8950b3778\n",
      "Removing intermediate container 89a8950b3778\n",
      " ---> c8b3185f23e5\n",
      "Step 3/14 : RUN mkdir -p $HOME/.cache\n",
      " ---> Running in 73324aee9f45\n",
      "Removing intermediate container 73324aee9f45\n",
      " ---> 052744da525b\n",
      "Step 4/14 : WORKDIR /\n",
      " ---> Running in 2c30c09f9d91\n",
      "Removing intermediate container 2c30c09f9d91\n",
      " ---> ce6d10f1d5b3\n",
      "Step 5/14 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
      " ---> 4f1661c53ab0\n",
      "Step 6/14 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\n",
      " ---> Running in 12aced4f19b8\n",
      "Removing intermediate container 12aced4f19b8\n",
      " ---> de4e602c8440\n",
      "Step 7/14 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\n",
      " ---> baec26682dd4\n",
      "Step 8/14 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3 -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \"$HOME/.cache/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\n",
      " ---> Running in 5b0ea7eae76c\n",
      "Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "Collecting package metadata (repodata.json): ...working... \n",
      "done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "sqlite-3.23.1        | 1.5 MB    |            |   0% \n",
      "sqlite-3.23.1        | 1.5 MB    | ####3      |  43% \n",
      "sqlite-3.23.1        | 1.5 MB    | ########## | 100% \n",
      "\n",
      "tk-8.6.10            | 3.2 MB    |            |   0% \n",
      "tk-8.6.10            | 3.2 MB    | ########## | 100% \n",
      "\n",
      "readline-7.0         | 387 KB    |            |   0% \n",
      "readline-7.0         | 387 KB    | ########## | 100% \n",
      "\n",
      "wheel-0.34.2         | 49 KB     |            |   0% \n",
      "wheel-0.34.2         | 49 KB     | ########## | 100% \n",
      "\n",
      "libffi-3.2.1         | 43 KB     |            |   0% \n",
      "libffi-3.2.1         | 43 KB     | ########## | 100% \n",
      "\n",
      "ncurses-6.0          | 907 KB    |            |   0% \n",
      "ncurses-6.0          | 907 KB    | ########## | 100% \n",
      "\n",
      "zlib-1.2.11          | 120 KB    |            |   0% \n",
      "zlib-1.2.11          | 120 KB    | ########## | 100% \n",
      "\n",
      "libedit-3.1          | 171 KB    |            |   0% \n",
      "libedit-3.1          | 171 KB    | ########## | 100% \n",
      "\n",
      "openssl-1.0.2u       | 3.1 MB    |            |   0% \n",
      "openssl-1.0.2u       | 3.1 MB    | ########## | 100% \n",
      "\n",
      "certifi-2020.6.20    | 160 KB    |            |   0% \n",
      "certifi-2020.6.20    | 160 KB    | ########## | 100% \n",
      "\n",
      "libgcc-ng-9.1.0      | 8.1 MB    |            |   0% \n",
      "libgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \n",
      "\n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    |            |   0% \n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    | ########## | 100% \n",
      "\n",
      "ca-certificates-2020 | 132 KB    |            |   0% \n",
      "ca-certificates-2020 | 132 KB    | ########## | 100% \n",
      "\n",
      "python-3.6.2         | 27.0 MB   |            |   0% \n",
      "python-3.6.2         | 27.0 MB   | ###1       |  32% \n",
      "python-3.6.2         | 27.0 MB   | #######1   |  71% \n",
      "python-3.6.2         | 27.0 MB   | ########## | 100% \n",
      "\n",
      "setuptools-47.3.1    | 647 KB    |            |   0% \n",
      "setuptools-47.3.1    | 647 KB    | ########## | 100% \n",
      "\n",
      "xz-5.2.5             | 438 KB    |            |   0% \n",
      "xz-5.2.5             | 438 KB    | ########## | 100% \n",
      "\n",
      "pip-20.1.1           | 2.0 MB    |            |   0% \n",
      "pip-20.1.1           | 2.0 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Ran pip subprocess with arguments:\n",
      "['/azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/bin/python', '-m', 'pip', 'install', '-U', '-r', '/azureml-environment-setup/condaenv.6rc_kcb4.requirements.txt']\n",
      "Pip subprocess output:\n",
      "Collecting azureml-dataprep[fuse,pandas]\n",
      "  Downloading azureml_dataprep-1.8.3-py3-none-any.whl (27.7 MB)\n",
      "Collecting tensorflow-gpu==2.2.0\n",
      "  Downloading tensorflow_gpu-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "Collecting tensorflow==2.2.0\n",
      "  Downloading tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2 MB)\n",
      "Collecting azureml-defaults\n",
      "  Downloading azureml_defaults-1.8.0-py3-none-any.whl (3.0 kB)\n",
      "Collecting horovod==0.19.1\n",
      "  Downloading horovod-0.19.1.tar.gz (2.9 MB)\n",
      "Collecting cloudpickle>=1.1.0\n",
      "  Downloading cloudpickle-1.4.1-py3-none-any.whl (26 kB)\n",
      "Collecting azureml-dataprep-native<15.0.0,>=14.2.1\n",
      "  Downloading azureml_dataprep_native-14.2.1-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting dotnetcore2>=2.1.14\n",
      "  Downloading dotnetcore2-2.1.14-py3-none-manylinux1_x86_64.whl (29.3 MB)\n",
      "Collecting azure-identity<1.3.0,>=1.2.0\n",
      "  Downloading azure_identity-1.2.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting fusepy>=3.0.1; extra == \"fuse\"\n",
      "  Downloading fusepy-3.0.1.tar.gz (11 kB)\n",
      "Collecting pandas>=0.23.4; extra == \"pandas\"\n",
      "  Downloading pandas-1.0.5-cp36-cp36m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting pyarrow>=0.15.*; extra == \"pandas\"\n",
      "  Downloading pyarrow-0.17.1-cp36-cp36m-manylinux2014_x86_64.whl (63.8 MB)\n",
      "Collecting numpy>=1.14.0; extra == \"pandas\"\n",
      "  Downloading numpy-1.19.0-cp36-cp36m-manylinux2010_x86_64.whl (14.6 MB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "Collecting six>=1.12.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/lib/python3.6/site-packages (from tensorflow-gpu==2.2.0->-r /azureml-environment-setup/condaenv.6rc_kcb4.requirements.txt (line 2)) (0.34.2)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting flask==1.0.3\n",
      "  Downloading Flask-1.0.3-py2.py3-none-any.whl (92 kB)\n",
      "Collecting azureml-core~=1.8.0\n",
      "  Downloading azureml_core-1.8.0-py3-none-any.whl (1.5 MB)\n",
      "Collecting configparser==3.7.4\n",
      "  Downloading configparser-3.7.4-py2.py3-none-any.whl (22 kB)\n",
      "Collecting json-logging-py==0.2\n",
      "  Downloading json-logging-py-0.2.tar.gz (3.6 kB)\n",
      "Collecting azureml-model-management-sdk==1.0.1b6.post1\n",
      "  Downloading azureml_model_management_sdk-1.0.1b6.post1-py2.py3-none-any.whl (130 kB)\n",
      "Collecting applicationinsights>=0.11.7\n",
      "  Downloading applicationinsights-0.11.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting gunicorn==19.9.0\n",
      "  Downloading gunicorn-19.9.0-py2.py3-none-any.whl (112 kB)\n",
      "Collecting werkzeug==0.16.1\n",
      "  Downloading Werkzeug-0.16.1-py2.py3-none-any.whl (327 kB)\n",
      "Collecting psutil\n",
      "  Downloading psutil-5.7.0.tar.gz (449 kB)\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "Collecting cffi>=1.4.0\n",
      "  Downloading cffi-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (399 kB)\n",
      "Collecting distro>=1.2.0\n",
      "  Downloading distro-1.5.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting azure-core<2.0.0,>=1.0.0\n",
      "  Downloading azure_core-1.6.0-py2.py3-none-any.whl (120 kB)\n",
      "Collecting msal<2.0.0,>=1.0.0\n",
      "  Downloading msal-1.4.0-py2.py3-none-any.whl (48 kB)\n",
      "Collecting cryptography>=2.1.4\n",
      "  Downloading cryptography-2.9.2-cp35-abi3-manylinux2010_x86_64.whl (2.7 MB)\n",
      "Collecting msal-extensions~=0.1.3\n",
      "  Downloading msal_extensions-0.1.3-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow-gpu==2.2.0->-r /azureml-environment-setup/condaenv.6rc_kcb4.requirements.txt (line 2)) (47.3.1.post20200622)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting Jinja2>=2.10\n",
      "  Downloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting click>=5.1\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Collecting azure-mgmt-network~=10.0\n",
      "  Downloading azure_mgmt_network-10.2.0-py2.py3-none-any.whl (8.6 MB)\n",
      "Collecting azure-mgmt-containerregistry>=2.0.0\n",
      "  Downloading azure_mgmt_containerregistry-2.8.0-py2.py3-none-any.whl (718 kB)\n",
      "Collecting docker\n",
      "  Downloading docker-4.2.1-py2.py3-none-any.whl (143 kB)\n",
      "Collecting azure-mgmt-keyvault>=0.40.0\n",
      "  Downloading azure_mgmt_keyvault-2.2.0-py2.py3-none-any.whl (89 kB)\n",
      "Collecting ndg-httpsclient\n",
      "  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\n",
      "Collecting contextlib2\n",
      "  Downloading contextlib2-0.6.0.post1-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting ruamel.yaml>0.16.7\n",
      "  Downloading ruamel.yaml-0.16.10-py2.py3-none-any.whl (111 kB)\n",
      "Collecting azure-graphrbac>=0.40.0\n",
      "  Downloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\n",
      "Collecting backports.tempfile\n",
      "  Downloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Collecting azure-mgmt-resource>=1.2.1\n",
      "  Downloading azure_mgmt_resource-10.0.0-py2.py3-none-any.whl (809 kB)\n",
      "Collecting urllib3>=1.23\n",
      "  Downloading urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "Collecting msrestazure>=0.4.33\n",
      "  Downloading msrestazure-0.6.3-py2.py3-none-any.whl (40 kB)\n",
      "Collecting PyJWT\n",
      "  Downloading PyJWT-1.7.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting jmespath\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting jsonpickle\n",
      "  Downloading jsonpickle-1.4.1-py2.py3-none-any.whl (36 kB)\n",
      "Collecting azure-mgmt-storage>=1.5.0\n",
      "  Downloading azure_mgmt_storage-11.0.0-py2.py3-none-any.whl (546 kB)\n",
      "Collecting msrest>=0.5.1\n",
      "  Downloading msrest-0.6.17-py2.py3-none-any.whl (84 kB)\n",
      "Collecting pathspec\n",
      "  Downloading pathspec-0.8.0-py2.py3-none-any.whl (28 kB)\n",
      "Collecting azure-mgmt-authorization>=0.40.0\n",
      "  Downloading azure_mgmt_authorization-0.60.0-py2.py3-none-any.whl (82 kB)\n",
      "Collecting pyopenssl\n",
      "  Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB)\n",
      "Collecting azure-common>=1.1.12\n",
      "  Downloading azure_common-1.1.25-py2.py3-none-any.whl (12 kB)\n",
      "Collecting SecretStorage\n",
      "  Downloading SecretStorage-3.1.2-py3-none-any.whl (14 kB)\n",
      "Collecting adal>=1.2.0\n",
      "  Downloading adal-1.2.4-py2.py3-none-any.whl (55 kB)\n",
      "Collecting liac-arff>=2.1.1\n",
      "  Downloading liac-arff-2.4.0.tar.gz (15 kB)\n",
      "Collecting dill>=0.2.7.1\n",
      "  Downloading dill-0.3.2.zip (177 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Collecting portalocker~=1.0\n",
      "  Downloading portalocker-1.7.0-py2.py3-none-any.whl (14 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-1.6.1-py2.py3-none-any.whl (31 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu==2.2.0->-r /azureml-environment-setup/condaenv.6rc_kcb4.requirements.txt (line 2)) (2020.6.20)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (27 kB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\n",
      "Collecting pyasn1>=0.1.1\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
      "  Downloading ruamel.yaml.clib-0.2.0-cp36-cp36m-manylinux1_x86_64.whl (548 kB)\n",
      "Collecting backports.weakref\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "Collecting jeepney>=0.4.2\n",
      "  Downloading jeepney-0.4.3-py3-none-any.whl (21 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
      "Building wheels for collected packages: horovod, fusepy, absl-py, wrapt, termcolor, json-logging-py, psutil, pyyaml, liac-arff, dill\n",
      "  Building wheel for horovod (setup.py): started\n",
      "  Building wheel for horovod (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for horovod\n",
      "  Building wheel for fusepy (setup.py): started\n",
      "  Building wheel for fusepy (setup.py): finished with status 'done'\n",
      "  Created wheel for fusepy: filename=fusepy-3.0.1-py3-none-any.whl size=10503 sha256=4370a0b7f4b4d18ed3b8092e7b30acfce6a73c3e1f96c1534a750290f3ec79b6\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/5c/83/1dd7e8a232d12227e5410120f4374b33adeb4037473105b079\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=7e81066d7d65df40f45c1a82d3ff07025588561265ce1708d6e5a191fd70bc2e\n",
      "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=69871 sha256=6c224d2124b28a185d8df70671bb9dfa8f38dafdf3a041d6197da673e6bf4094\n",
      "  Stored in directory: /root/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=b72af73f6b52bdfba944c809a09168c52c75fd81e47bcc15316b7a31a80c04cb\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for json-logging-py (setup.py): started\n",
      "  Building wheel for json-logging-py (setup.py): finished with status 'done'\n",
      "  Created wheel for json-logging-py: filename=json_logging_py-0.2-py3-none-any.whl size=3923 sha256=1780f3b0dea0af5d8d0ab11a100f83e3d71ff799160520dc8feb4115f6e998e4\n",
      "  Stored in directory: /root/.cache/pip/wheels/e2/1d/52/535a274b9c2ce7d4064838f2bdb62013801281ef7d7f21e2ee\n",
      "  Building wheel for psutil (setup.py): started\n",
      "  Building wheel for psutil (setup.py): finished with status 'done'\n",
      "  Created wheel for psutil: filename=psutil-5.7.0-cp36-cp36m-linux_x86_64.whl size=272962 sha256=c66dc513fd718d6620f63142e71f124aa27c0a5a8754e1e702512a8db8cba05a\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/d9/f2/b5620c01e9b3e858c6877b1045fda5b115cf7df6490f883382\n",
      "  Building wheel for pyyaml (setup.py): started\n",
      "  Building wheel for pyyaml (setup.py): finished with status 'done'\n",
      "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=ca02e12506cbe71acf4fe54f092e146e7ac25f997eeb325730cf946d9b9173f9\n",
      "  Stored in directory: /root/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc\n",
      "  Building wheel for liac-arff (setup.py): started\n",
      "  Building wheel for liac-arff (setup.py): finished with status 'done'\n",
      "  Created wheel for liac-arff: filename=liac_arff-2.4.0-py3-none-any.whl size=13333 sha256=7101a35b313bf6c6cdd18b037e9415162f1f682a7c0cedcac66c18f6ed3e0ae7\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/2a/e1/6f7be2e2ea150e2486bff64fd6f0670f4f35f4c8f31c819fb8\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78912 sha256=0c6e628f02fea04c25d3eb874c59ca83da1c7246929813374adf5208e5f4a3d2\n",
      "  Stored in directory: /root/.cache/pip/wheels/02/49/cf/660924cd9bc5fcddc3a0246fe39800c83028d3ccea244de352\n",
      "Successfully built fusepy absl-py wrapt termcolor json-logging-py psutil pyyaml liac-arff dill\n",
      "Failed to build horovod\n",
      "Installing collected packages: cloudpickle, azureml-dataprep-native, distro, dotnetcore2, six, idna, urllib3, chardet, requests, azure-core, PyJWT, msal, pycparser, cffi, cryptography, portalocker, msal-extensions, azure-identity, fusepy, pytz, numpy, python-dateutil, pandas, pyarrow, azureml-dataprep, absl-py, scipy, protobuf, keras-preprocessing, astunparse, wrapt, h5py, google-pasta, grpcio, tensorboard-plugin-wit, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, zipp, importlib-metadata, markdown, werkzeug, tensorboard, opt-einsum, tensorflow-estimator, gast, termcolor, tensorflow-gpu, tensorflow, MarkupSafe, Jinja2, itsdangerous, click, flask, azure-common, isodate, msrest, adal, msrestazure, azure-mgmt-network, azure-mgmt-containerregistry, websocket-client, docker, azure-mgmt-keyvault, pyopenssl, ndg-httpsclient, contextlib2, ruamel.yaml.clib, ruamel.yaml, azure-graphrbac, backports.weakref, backports.tempfile, azure-mgmt-resource, jmespath, jsonpickle, azure-mgmt-storage, pathspec, azure-mgmt-authorization, jeepney, SecretStorage, azureml-core, configparser, json-logging-py, liac-arff, dill, azureml-model-management-sdk, applicationinsights, gunicorn, azureml-defaults, psutil, pyyaml, horovod\n",
      "    Running setup.py install for horovod: started\n",
      "    Running setup.py install for horovod: still running...\n",
      "    Running setup.py install for horovod: finished with status 'done'\n",
      "Successfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 PyJWT-1.7.1 SecretStorage-3.1.2 absl-py-0.9.0 adal-1.2.4 applicationinsights-0.11.9 astunparse-1.6.3 azure-common-1.1.25 azure-core-1.6.0 azure-graphrbac-0.61.1 azure-identity-1.2.0 azure-mgmt-authorization-0.60.0 azure-mgmt-containerregistry-2.8.0 azure-mgmt-keyvault-2.2.0 azure-mgmt-network-10.2.0 azure-mgmt-resource-10.0.0 azure-mgmt-storage-11.0.0 azureml-core-1.8.0 azureml-dataprep-1.8.3 azureml-dataprep-native-14.2.1 azureml-defaults-1.8.0 azureml-model-management-sdk-1.0.1b6.post1 backports.tempfile-1.0 backports.weakref-1.0.post1 cachetools-4.1.0 cffi-1.14.0 chardet-3.0.4 click-7.1.2 cloudpickle-1.4.1 configparser-3.7.4 contextlib2-0.6.0.post1 cryptography-2.9.2 dill-0.3.2 distro-1.5.0 docker-4.2.1 dotnetcore2-2.1.14 flask-1.0.3 fusepy-3.0.1 gast-0.3.3 google-auth-1.18.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 gunicorn-19.9.0 h5py-2.10.0 horovod-0.19.1 idna-2.9 importlib-metadata-1.6.1 isodate-0.6.0 itsdangerous-1.1.0 jeepney-0.4.3 jmespath-0.10.0 json-logging-py-0.2 jsonpickle-1.4.1 keras-preprocessing-1.1.2 liac-arff-2.4.0 markdown-3.2.2 msal-1.4.0 msal-extensions-0.1.3 msrest-0.6.17 msrestazure-0.6.3 ndg-httpsclient-0.5.1 numpy-1.19.0 oauthlib-3.1.0 opt-einsum-3.2.1 pandas-1.0.5 pathspec-0.8.0 portalocker-1.7.0 protobuf-3.12.2 psutil-5.7.0 pyarrow-0.17.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.20 pyopenssl-19.1.0 python-dateutil-2.8.1 pytz-2020.1 pyyaml-5.3.1 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 ruamel.yaml-0.16.10 ruamel.yaml.clib-0.2.0 scipy-1.4.1 six-1.15.0 tensorboard-2.2.2 tensorboard-plugin-wit-1.6.0.post3 tensorflow-2.2.0 tensorflow-estimator-2.2.0 tensorflow-gpu-2.2.0 termcolor-1.1.0 urllib3-1.25.9 websocket-client-0.57.0 werkzeug-0.16.1 wrapt-1.12.1 zipp-3.1.0\n",
      "\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.12\n",
      "  latest version: 4.8.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "WARNING: /root/.conda/pkgs does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing intermediate container 5b0ea7eae76c\n",
      " ---> a19b108bc66b\n",
      "Step 9/14 : ENV PATH /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/bin:$PATH\n",
      " ---> Running in f327f3a40a32\n",
      "Removing intermediate container f327f3a40a32\n",
      " ---> 88e1ad5c616d\n",
      "Step 10/14 : ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3\n",
      " ---> Running in c12b5e45020f\n",
      "Removing intermediate container c12b5e45020f\n",
      " ---> f05059940df0\n",
      "Step 11/14 : ENV LD_LIBRARY_PATH /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/lib:$LD_LIBRARY_PATH\n",
      " ---> Running in 0e0b24b8f199\n",
      "Removing intermediate container 0e0b24b8f199\n",
      " ---> e02a846efa81\n",
      "Step 12/14 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\n",
      " ---> 1600a8f22c28\n",
      "Step 13/14 : ENV AZUREML_ENVIRONMENT_IMAGE True\n",
      " ---> Running in 820090f09735\n",
      "Removing intermediate container 820090f09735\n",
      " ---> 5fb765e2f35e\n",
      "Step 14/14 : CMD [\"bash\"]\n",
      " ---> Running in c2b012564a92\n",
      "Removing intermediate container c2b012564a92\n",
      " ---> e37f6b21e95d\n",
      "Successfully built e37f6b21e95d\n",
      "Successfully tagged wu2modtimesmls5288786732.azurecr.io/azureml/azureml_bea5976044066c2c279dba90c6eb05e8:latest\n",
      "2020/06/25 23:35:40 Successfully executed container: acb_step_0\n",
      "2020/06/25 23:35:40 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2020/06/25 23:35:40 Pushing image: wu2modtimesmls5288786732.azurecr.io/azureml/azureml_bea5976044066c2c279dba90c6eb05e8:latest, attempt 1\n",
      "The push refers to repository [wu2modtimesmls5288786732.azurecr.io/azureml/azureml_bea5976044066c2c279dba90c6eb05e8]\n",
      "ef50364bc95b: Preparing\n",
      "446881c624f1: Preparing\n",
      "e17bf0e82218: Preparing\n",
      "a11b36e5e949: Preparing\n",
      "4f5ca96b405f: Preparing\n",
      "2d653dedb49b: Preparing\n",
      "ab22777f96ca: Preparing\n",
      "8c18c12c195a: Preparing\n",
      "f15f63e5481e: Preparing\n",
      "404eb2d334b1: Preparing\n",
      "4c6885a13185: Preparing\n",
      "ae93d3a40a63: Preparing\n",
      "ed8243491c94: Preparing\n",
      "81e535525773: Preparing\n",
      "582ab80c9f26: Preparing\n",
      "4e3516398cef: Preparing\n",
      "52ad947270f1: Preparing\n",
      "dd841c774a30: Preparing\n",
      "37b9a4b22186: Preparing\n",
      "e0b3afb09dc3: Preparing\n",
      "6c01b5a53aac: Preparing\n",
      "2c6ac8e5063e: Preparing\n",
      "cc967c529ced: Preparing\n",
      "2d653dedb49b: Waiting\n",
      "ab22777f96ca: Waiting\n",
      "8c18c12c195a: Waiting\n",
      "f15f63e5481e: Waiting\n",
      "404eb2d334b1: Waiting\n",
      "4c6885a13185: Waiting\n",
      "ae93d3a40a63: Waiting\n",
      "ed8243491c94: Waiting\n",
      "81e535525773: Waiting\n",
      "582ab80c9f26: Waiting\n",
      "4e3516398cef: Waiting\n",
      "52ad947270f1: Waiting\n",
      "dd841c774a30: Waiting\n",
      "37b9a4b22186: Waiting\n",
      "e0b3afb09dc3: Waiting\n",
      "6c01b5a53aac: Waiting\n",
      "2c6ac8e5063e: Waiting\n",
      "cc967c529ced: Waiting\n",
      "a11b36e5e949: Pushed\n",
      "4f5ca96b405f: Pushed\n",
      "e17bf0e82218: Pushed\n",
      "ef50364bc95b: Pushed\n",
      "2d653dedb49b: Pushed\n",
      "ab22777f96ca: Pushed\n",
      "8c18c12c195a: Pushed\n",
      "f15f63e5481e: Pushed\n",
      "4c6885a13185: Pushed\n",
      "ae93d3a40a63: Pushed\n",
      "ed8243491c94: Pushed\n",
      "404eb2d334b1: Pushed\n",
      "52ad947270f1: Pushed\n",
      "dd841c774a30: Pushed\n",
      "37b9a4b22186: Pushed\n",
      "e0b3afb09dc3: Pushed\n",
      "6c01b5a53aac: Pushed\n",
      "2c6ac8e5063e: Pushed\n",
      "81e535525773: Pushed\n",
      "cc967c529ced: Pushed\n",
      "4e3516398cef: Pushed\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_dec38e26a23aa07bf563c13e9e3e5a7f583b1f5e22bda170dcf8418d4f72e828_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2020-06-25T23:40:35Z Starting output-watcher...\n",
      "2020-06-25T23:40:35Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Entering context manager injector. Current time:2020-06-25T23:42:38.070962\n",
      "Initialize DatasetContextManager.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 108\n",
      "Set Dataset mnist's target path to /tmp/tmp03me2ac0\n",
      "Enter __enter__ of DatasetContextManager\n",
      "SDK version: azureml-core==1.8.0 azureml-dataprep==1.8.3\n",
      "Processing 'mnist'\n",
      "Processing dataset FileDataset\n",
      "{\n",
      "  \"source\": [\n",
      "    \"https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz\",\n",
      "    \"https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz\",\n",
      "    \"https://azureopendatastorage.blob.core.windows.net/mnist/t10k-images-idx3-ubyte.gz\",\n",
      "    \"https://azureopendatastorage.blob.core.windows.net/mnist/t10k-labels-idx1-ubyte.gz\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetFiles\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"ee73c186-95ee-44f1-9577-c5dd16f12d8d\",\n",
      "    \"name\": \"mnist-dataset\",\n",
      "    \"version\": 1,\n",
      "    \"description\": \"training and test dataset\",\n",
      "    \"workspace\": \"Workspace.create(name='wu2modtimesmlsw', subscription_id='7fd76d0f-84f2-498b-a997-e0d059af5ce1', resource_group='wu2modtimerg')\"\n",
      "  }\n",
      "}\n",
      "Mounting mnist to /tmp/tmp03me2ac0\n",
      "Mounted mnist to /tmp/tmp03me2ac0\n",
      "Exit __enter__ of DatasetContextManager\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ tf_mnist.py ] with arguments: ['--data-folder', '$mnist', '--batch-size', '64', '--first-layer-neurons', '256', '--second-layer-neurons', '128', '--learning-rate', '0.01']\n",
      "After variable expansion, calling script [ tf_mnist.py ] with arguments: ['--data-folder', '/tmp/tmp03me2ac0', '--batch-size', '64', '--first-layer-neurons', '256', '--second-layer-neurons', '128', '--learning-rate', '0.01']\n",
      "\n",
      "TensorFlow version: 2.2.0\n",
      "Data folder: /tmp/tmp03me2ac0\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n",
      "2020-06-25 23:43:09.309401: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2020-06-25 23:43:09.339715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 3ae7:00:00.0 name: Tesla K80 computeCapability: 3.7\n",
      "coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n",
      "2020-06-25 23:43:09.340145: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/lib/python3.6/site-packages/dotnetcore2/bin/deps\n",
      "2020-06-25 23:43:09.340397: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/lib/python3.6/site-packages/dotnetcore2/bin/deps\n",
      "2020-06-25 23:43:09.340592: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/lib/python3.6/site-packages/dotnetcore2/bin/deps\n",
      "2020-06-25 23:43:09.340781: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/lib/python3.6/site-packages/dotnetcore2/bin/deps\n",
      "2020-06-25 23:43:09.340966: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/lib/python3.6/site-packages/dotnetcore2/bin/deps\n",
      "2020-06-25 23:43:09.341161: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_7fd238c9cddda6fa149db3c75a94ccc3/lib/python3.6/site-packages/dotnetcore2/bin/deps\n",
      "2020-06-25 23:43:09.345444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2020-06-25 23:43:09.345499: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2020-06-25 23:43:09.345872: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2020-06-25 23:43:09.352776: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2596990000 Hz\n",
      "2020-06-25 23:43:09.353143: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6664000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-06-25 23:43:09.353208: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-06-25 23:43:09.354618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2020-06-25 23:43:09.354676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      \n",
      "0 -- Training accuracy: tf.Tensor(0.921875, shape=(), dtype=float32) Validation accuracy: tf.Tensor(0.8962, shape=(), dtype=float32)\n",
      "1 -- Training accuracy: tf.Tensor(0.953125, shape=(), dtype=float32) Validation accuracy: tf.Tensor(0.9155, shape=(), dtype=float32)\n",
      "2 -- Training accuracy: tf.Tensor(0.9375, shape=(), dtype=float32) Validation accuracy: tf.Tensor(0.9265, shape=(), dtype=float32)\n",
      "3 -- Training accuracy: tf.Tensor(0.953125, shape=(), dtype=float32) Validation accuracy: tf.Tensor(0.9331, shape=(), dtype=float32)\n",
      "4 -- Training accuracy: tf.Tensor(0.984375, shape=(), dtype=float32) Validation accuracy: tf.Tensor(0.938, shape=(), dtype=float32)\n",
      "5 -- Training accuracy: tf.Tensor(0.90625, shape=(), dtype=float32) Validation accuracy: tf.Tensor(0.9418, shape=(), dtype=float32)\n",
      "6 -- Training accuracy: tf.Tensor(0.9375, shape=(), dtype=float32) Validation accuracy: tf.Tensor(0.9467, shape=(), dtype=float32)\n",
      "7 -- Training accuracy: tf.Tensor(0.984375, shape=(), dtype=float32) Validation accuracy: tf.Tensor(0.95, shape=(), dtype=float32)\n",
      "8 -- Training accuracy: tf.Tensor(0.953125, shape=(), dtype=float32) Validation accuracy: tf.Tensor(0.9515, shape=(), dtype=float32)\n",
      "9 -- Training accuracy: tf.Tensor(0.9375, shape=(), dtype=float32) Validation accuracy: tf.Tensor(0.9531, shape=(), dtype=float32)\n",
      "10 -- Training accuracy: tf.Tensor(0.984375, shape=(), dtype=float32) Validation accuracy: tf.Tensor(0.9568, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_dec38e26a23aa07bf563c13e9e3e5a7f583b1f5e22bda170dcf8418d4f72e828_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "Entering job release. Current time:2020-06-25T23:44:53.045891\n",
      "Starting job release. Current time:2020-06-25T23:44:53.848062\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 628\n",
      "Entering context manager injector. Current time:2020-06-25T23:44:53.867204\n",
      "Job release is complete. Current time:2020-06-25T23:44:57.439965\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: tf-mnist_1593127650_217d3343\n",
      "Web View: https://ml.azure.com/experiments/tf-mnist/runs/tf-mnist_1593127650_217d3343?wsid=/subscriptions/7fd76d0f-84f2-498b-a997-e0d059af5ce1/resourcegroups/wu2modtimerg/workspaces/wu2modtimesmlsw\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'tf-mnist_1593127650_217d3343',\n",
       " 'target': 'gpu-cluster',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2020-06-25T23:40:38.7312Z',\n",
       " 'endTimeUtc': '2020-06-25T23:45:05.342766Z',\n",
       " 'properties': {'_azureml.ComputeTargetType': 'amlcompute',\n",
       "  'ContentSnapshotId': '28da1ffc-b78c-40e4-bb86-820d8b30730e',\n",
       "  'azureml.git.repository_uri': 'https://github.com/Azure/MachineLearningNotebooks.git',\n",
       "  'mlflow.source.git.repoURL': 'https://github.com/Azure/MachineLearningNotebooks.git',\n",
       "  'azureml.git.branch': 'master',\n",
       "  'mlflow.source.git.branch': 'master',\n",
       "  'azureml.git.commit': '053efde8c9740c7b691c4d13ee1f5b5b206cd24f',\n",
       "  'mlflow.source.git.commit': '053efde8c9740c7b691c4d13ee1f5b5b206cd24f',\n",
       "  'azureml.git.dirty': 'True',\n",
       "  'ProcessInfoFile': 'azureml-logs/process_info.json',\n",
       "  'ProcessStatusFile': 'azureml-logs/process_status.json'},\n",
       " 'inputDatasets': [{'dataset': {'id': 'ee73c186-95ee-44f1-9577-c5dd16f12d8d'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'mnist', 'mechanism': 'Mount'}}],\n",
       " 'runDefinition': {'script': 'tf_mnist.py',\n",
       "  'useAbsolutePath': False,\n",
       "  'arguments': ['--data-folder',\n",
       "   'DatasetConsumptionConfig:mnist',\n",
       "   '--batch-size',\n",
       "   '64',\n",
       "   '--first-layer-neurons',\n",
       "   '256',\n",
       "   '--second-layer-neurons',\n",
       "   '128',\n",
       "   '--learning-rate',\n",
       "   '0.01'],\n",
       "  'sourceDirectoryDataStore': None,\n",
       "  'framework': 'Python',\n",
       "  'communicator': 'None',\n",
       "  'target': 'gpu-cluster',\n",
       "  'dataReferences': {},\n",
       "  'data': {'mnist': {'dataLocation': {'dataset': {'id': 'ee73c186-95ee-44f1-9577-c5dd16f12d8d',\n",
       "      'name': None,\n",
       "      'version': None},\n",
       "     'dataPath': None},\n",
       "    'mechanism': 'Mount',\n",
       "    'environmentVariableName': 'mnist',\n",
       "    'pathOnCompute': None,\n",
       "    'overwrite': False}},\n",
       "  'outputData': {},\n",
       "  'jobName': None,\n",
       "  'maxRunDurationSeconds': None,\n",
       "  'nodeCount': 1,\n",
       "  'environment': {'name': 'Experiment tf-mnist Environment',\n",
       "   'version': 'Autosave_2020-06-25T23:27:33Z_cc4c17a7',\n",
       "   'python': {'interpreterPath': 'python',\n",
       "    'userManagedDependencies': False,\n",
       "    'condaDependencies': {'channels': ['anaconda', 'conda-forge'],\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-dataprep[pandas,fuse]',\n",
       "        'tensorflow-gpu==2.2.0',\n",
       "        'tensorflow==2.2.0',\n",
       "        'azureml-defaults',\n",
       "        'horovod==0.19.1']}],\n",
       "     'name': 'azureml_7fd238c9cddda6fa149db3c75a94ccc3'},\n",
       "    'baseCondaEnvironment': None},\n",
       "   'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.0-cudnn7-ubuntu18.04:20200423.v1',\n",
       "    'platform': {'os': 'Linux', 'architecture': 'amd64'},\n",
       "    'baseDockerfile': None,\n",
       "    'baseImageRegistry': {'address': None, 'username': None, 'password': None},\n",
       "    'enabled': True,\n",
       "    'arguments': []},\n",
       "   'spark': {'repositories': [], 'packages': [], 'precachePackages': False},\n",
       "   'inferencingStackVersion': None},\n",
       "  'history': {'outputCollection': True,\n",
       "   'directoriesToWatch': ['logs'],\n",
       "   'snapshotProject': True},\n",
       "  'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'parallelTask': {'maxRetriesPerWorker': 0,\n",
       "   'workerCountPerNode': 1,\n",
       "   'terminalExitCodes': None,\n",
       "   'configuration': {}},\n",
       "  'amlCompute': {'name': None,\n",
       "   'vmSize': None,\n",
       "   'retainCluster': False,\n",
       "   'clusterMaxNodeCount': 1},\n",
       "  'tensorflow': {'workerCount': 1, 'parameterServerCount': 1},\n",
       "  'mpi': {'processCountPerNode': 1},\n",
       "  'hdi': {'yarnDeployMode': 'Cluster'},\n",
       "  'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5},\n",
       "  'exposedPorts': None,\n",
       "  'docker': {'useDocker': True,\n",
       "   'sharedVolumes': True,\n",
       "   'shmSize': '2g',\n",
       "   'arguments': []},\n",
       "  'cmk8sCompute': {'configuration': {}},\n",
       "  'itpCompute': {'configuration': {}},\n",
       "  'cmAksCompute': {'configuration': {}}},\n",
       " 'logFiles': {'azureml-logs/20_image_build_log.txt': 'https://wu2modtimesmls2858026735.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1593127650_217d3343/azureml-logs/20_image_build_log.txt?sv=2019-02-02&sr=b&sig=ehl4eeShjYYU3DcWuNp8gwW1PanTFQsmthepJofGSTc%3D&st=2020-06-25T23%3A35%3A08Z&se=2020-06-26T07%3A45%3A08Z&sp=r',\n",
       "  'azureml-logs/55_azureml-execution-tvmps_dec38e26a23aa07bf563c13e9e3e5a7f583b1f5e22bda170dcf8418d4f72e828_d.txt': 'https://wu2modtimesmls2858026735.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1593127650_217d3343/azureml-logs/55_azureml-execution-tvmps_dec38e26a23aa07bf563c13e9e3e5a7f583b1f5e22bda170dcf8418d4f72e828_d.txt?sv=2019-02-02&sr=b&sig=IqA3FERDSXvSJxUkTYHplxQAnMpvO2CyLUegZE18qOA%3D&st=2020-06-25T23%3A35%3A08Z&se=2020-06-26T07%3A45%3A08Z&sp=r',\n",
       "  'azureml-logs/65_job_prep-tvmps_dec38e26a23aa07bf563c13e9e3e5a7f583b1f5e22bda170dcf8418d4f72e828_d.txt': 'https://wu2modtimesmls2858026735.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1593127650_217d3343/azureml-logs/65_job_prep-tvmps_dec38e26a23aa07bf563c13e9e3e5a7f583b1f5e22bda170dcf8418d4f72e828_d.txt?sv=2019-02-02&sr=b&sig=KgNpu4gOXGV4sMtDTkfzbxBYTuC1I6s%2FQMCPpwEZyyE%3D&st=2020-06-25T23%3A35%3A08Z&se=2020-06-26T07%3A45%3A08Z&sp=r',\n",
       "  'azureml-logs/70_driver_log.txt': 'https://wu2modtimesmls2858026735.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1593127650_217d3343/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=yr2EBXz5BYXfMX6mOJ%2FyyKscFlSREVjanW9c749iOxw%3D&st=2020-06-25T23%3A35%3A08Z&se=2020-06-26T07%3A45%3A08Z&sp=r',\n",
       "  'azureml-logs/75_job_post-tvmps_dec38e26a23aa07bf563c13e9e3e5a7f583b1f5e22bda170dcf8418d4f72e828_d.txt': 'https://wu2modtimesmls2858026735.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1593127650_217d3343/azureml-logs/75_job_post-tvmps_dec38e26a23aa07bf563c13e9e3e5a7f583b1f5e22bda170dcf8418d4f72e828_d.txt?sv=2019-02-02&sr=b&sig=HbstJVZgQjylxhT5Vd9zxlUBEtyTVA7OVK54k1rT7ik%3D&st=2020-06-25T23%3A35%3A08Z&se=2020-06-26T07%3A45%3A08Z&sp=r',\n",
       "  'azureml-logs/process_info.json': 'https://wu2modtimesmls2858026735.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1593127650_217d3343/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=CDp184QNL06KQTnjO9zDk%2B4%2F%2Fh6bPaRjG5z%2BxiQiB8w%3D&st=2020-06-25T23%3A35%3A08Z&se=2020-06-26T07%3A45%3A08Z&sp=r',\n",
       "  'azureml-logs/process_status.json': 'https://wu2modtimesmls2858026735.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1593127650_217d3343/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=G%2BegBHAn4ZOv8ABQkXMRGMZ6n6E%2Fp4nvqtX6A%2BJThps%3D&st=2020-06-25T23%3A35%3A08Z&se=2020-06-26T07%3A45%3A08Z&sp=r',\n",
       "  'logs/azureml/108_azureml.log': 'https://wu2modtimesmls2858026735.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1593127650_217d3343/logs/azureml/108_azureml.log?sv=2019-02-02&sr=b&sig=ecvvnuNKykBxXBawuVS4DFjAdOA8aDgv21Rvu6OMpco%3D&st=2020-06-25T23%3A35%3A08Z&se=2020-06-26T07%3A45%3A08Z&sp=r',\n",
       "  'logs/azureml/job_prep_azureml.log': 'https://wu2modtimesmls2858026735.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1593127650_217d3343/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=ecJ6NfXDcaqbYMaxmNb4AVOjC0a%2BiZ%2BNNhUepZNlUQY%3D&st=2020-06-25T23%3A35%3A08Z&se=2020-06-26T07%3A45%3A08Z&sp=r',\n",
       "  'logs/azureml/job_release_azureml.log': 'https://wu2modtimesmls2858026735.blob.core.windows.net/azureml/ExperimentRun/dcid.tf-mnist_1593127650_217d3343/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=Mq1%2BtNtWsdhnGKJlTgUY%2Bn4CzUc3Vv7Ljt%2FYLW3NB0M%3D&st=2020-06-25T23%3A35%3A08Z&se=2020-06-26T07%3A45%3A08Z&sp=r'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Run object <a class=\"anchor\" id=\"run-object\"></a>\n",
    "The Run object provides the interface to the run history -- both to the job and to the control plane (this notebook), and both while the job is running and after it has completed. It provides a number of interesting features for instance:\n",
    "* `run.get_details()`: Provides a rich set of properties of the run\n",
    "* `run.get_metrics()`: Provides a dictionary with all the metrics that were reported for the Run\n",
    "* `run.get_file_names()`: List all the files that were uploaded to the run history for this Run. This will include the `outputs` and `logs` folder, azureml-logs and other logs, as well as files that were explicitly uploaded to the run using `run.upload_file()`\n",
    "\n",
    "Below are some examples -- please run through them and inspect their output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracy over epochs\n",
    "Since we can retrieve the metrics from the run, we can easily make plots using `matplotlib` in the notebook. Then we can add the plotted image to the run using `run.log_image()`, so all information about the run is kept together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs('./imgs', exist_ok=True)\n",
    "metrics = run.get_metrics()\n",
    "\n",
    "plt.figure(figsize = (13,5))\n",
    "plt.plot(metrics['validation_acc'], 'r-', lw=4, alpha=.6)\n",
    "plt.plot(metrics['training_acc'], 'b--', alpha=0.5)\n",
    "plt.legend(['Full evaluation set', 'Training set mini-batch'])\n",
    "plt.xlabel('epochs', fontsize=14)\n",
    "plt.ylabel('accuracy', fontsize=14)\n",
    "plt.title('Accuracy over Epochs', fontsize=16)\n",
    "run.log_image(name='acc_over_epochs.png', plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training script, a TensorFlow `saver` object is used to persist the model in a local folder (local to the compute target). The model was saved to the `./outputs` folder on the disk of the Batch AI cluster node where the job is run. Azure ML automatically uploaded anything written in the `./outputs` folder into run history file store. Subsequently, we can use the `Run` object to download the model files the `saver` object saved. They are under the the `outputs/model` folder in the run history file store, and are downloaded into a local folder named `model`. Note the TensorFlow model consists of four files in binary format and they are not human-readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_files(prefix='outputs/model', output_directory='./model', append_prefix=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the test set\n",
    "Now load the saved TensorFlow graph, and list all operations under the `network` scope. This way we can discover the input tensor `network/X:0` and the output tensor `network/output/MatMul:0`, and use them in the scoring script in the next step.\n",
    "\n",
    "Note: if your local TensorFlow version is different than the version running in the cluster where the model is trained, you might see a \"compiletime version mismatch\" warning. You can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "imported_model = tf.saved_model.load('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred =imported_model(X_test)\n",
    "y_hat = np.argmax(pred, axis=1)\n",
    "\n",
    "# print the first 30 labels and predictions\n",
    "print('labels:  \\t', y_test[:30])\n",
    "print('predictions:\\t', y_hat[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on the test set:\", np.average(y_hat == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on the test set:\", np.average(y_hat == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intelligent hyperparameter tuning\n",
    "We have trained the model with one set of hyperparameters, now let's how we can do hyperparameter tuning by launching multiple runs on the cluster. First let's define the parameter space using random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "\n",
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch-size': choice(32, 64, 128),\n",
    "        '--first-layer-neurons': choice(16, 64, 128, 256, 512),\n",
    "        '--second-layer-neurons': choice(16, 64, 256, 512),\n",
    "        '--learning-rate': loguniform(-6, -1)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a new estimator without the above parameters since they will be passed in later. Note we still need to keep the `data-folder` parameter since that's not a hyperparamter we will sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params={'--data-folder': dataset.as_named_input('mnist').as_mount()},\n",
    "                 compute_target=compute_target,\n",
    "                 entry_script='tf_mnist.py',\n",
    "                 framework_version='2.0',\n",
    "                 use_gpu=True,\n",
    "                 pip_packages=['azureml-dataprep[pandas,fuse]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define an early termnination policy. The `BanditPolicy` basically states to check the job every 2 iterations. If the primary metric (defined later) falls outside of the top 10% range, Azure ML terminate the job. This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to configure a run configuration object, and specify the primary metric `validation_acc` that's recorded in your training runs. If you go back to visit the training script, you will notice that this value is being logged after every epoch (a full batch set). We also want to tell the service that we are looking to maximizing this value. We also set the number of samples to 20, and maximal concurrent job to 4, which is the same as the number of nodes in our computer cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htc = HyperDriveConfig(estimator=est, \n",
    "                       hyperparameter_sampling=ps, \n",
    "                       policy=policy, \n",
    "                       primary_metric_name='validation_acc', \n",
    "                       primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                       max_total_runs=8,\n",
    "                       max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's launch the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htr = exp.submit(config=htc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a run history widget to show the progress. Be patient as this might take a while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(htr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htr.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(htr.get_status() == \"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm start a Hyperparameter Tuning experiment and resuming child runs\n",
    "Often times, finding the best hyperparameter values for your model can be an iterative process, needing multiple tuning runs that learn from previous hyperparameter tuning runs. Reusing knowledge from these previous runs will accelerate the hyperparameter tuning process, thereby reducing the cost of tuning the model and will potentially improve the primary metric of the resulting model. When warm starting a hyperparameter tuning experiment with Bayesian sampling, trials from the previous run will be used as prior knowledge to intelligently pick new samples, so as to improve the primary metric. Additionally, when using Random or Grid sampling, any early termination decisions will leverage metrics from the previous runs to determine poorly performing training runs. \n",
    "\n",
    "Azure Machine Learning allows you to warm start your hyperparameter tuning run by leveraging knowledge from up to 5 previously completed hyperparameter tuning parent runs. \n",
    "\n",
    "Additionally, there might be occasions when individual training runs of a hyperparameter tuning experiment are cancelled due to budget constraints or fail due to other reasons. It is now possible to resume such individual training runs from the last checkpoint (assuming your training script handles checkpoints). Resuming an individual training run will use the same hyperparameter configuration and mount the storage used for that run. The training script should accept the \"--resume-from\" argument, which contains the checkpoint or model files from which to resume the training run. You can also resume individual runs as part of an experiment that spends additional budget on hyperparameter tuning. Any additional budget, after resuming the specified training runs is used for exploring additional configurations.\n",
    "\n",
    "For more information on warm starting and resuming hyperparameter tuning runs, please refer to the [Hyperparameter Tuning for Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) \n",
    "\n",
    "## Find and register best model <a class=\"anchor\" id=\"register-model\"></a>\n",
    "When all the jobs finish, we can find out the one that has the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = htr.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's list the model files uploaded during the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then register the folder (and all files in it) as a model named `tf-dnn-mnist` under the workspace for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='tf-dnn-mnist', model_path='outputs/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model in ACI\n",
    "Now we are ready to deploy the model as a web service running in Azure Container Instance [ACI](https://azure.microsoft.com/en-us/services/container-instances/). Azure Machine Learning accomplishes this by constructing a Docker image with the scoring logic and model baked in.\n",
    "### Create score.py\n",
    "First, we will create a scoring script that will be invoked by the web service call. \n",
    "\n",
    "* Note that the scoring script must have two required functions, `init()` and `run(input_data)`. \n",
    "  * In `init()` function, you typically load the model into a global object. This function is executed only once when the Docker container is started. \n",
    "  * In `run(input_data)` function, the model is used to predict a value based on the input data. The input and output to `run` typically use JSON as serialization and de-serialization format but you are not limited to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global tf_model\n",
    "    model_root = os.getenv('AZUREML_MODEL_DIR')\n",
    "    # the name of the folder in which to look for tensorflow model files\n",
    "    tf_model_folder = 'model'\n",
    "    \n",
    "    tf_model = tf.saved_model.load(os.path.join(model_root, tf_model_folder))\n",
    "\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'], dtype=np.float32)\n",
    "    \n",
    "    # make prediction\n",
    "    out = tf_model(data)\n",
    "    y_hat = np.argmax(out, axis=1)\n",
    "\n",
    "    return y_hat.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create myenv.yml\n",
    "We also need to create an environment file so that Azure Machine Learning can install the necessary packages in the Docker image which are required by your scoring script. In this case, we need to specify packages `numpy`, `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "cd = CondaDependencies.create()\n",
    "cd.add_conda_package('numpy')\n",
    "cd.add_pip_package('tensorflow==2.0.0')\n",
    "cd.add_pip_package(\"azureml-defaults\")\n",
    "cd.save_to_file(base_directory='./', conda_file_path='myenv.yml')\n",
    "\n",
    "print(cd.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to ACI\n",
    "We are almost ready to deploy. Create the inference configuration and deployment configuration and deploy to ACI. This cell will run for about 7-8 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={'name':'mnist', 'framework': 'TensorFlow DNN'},\n",
    "                                               description='Tensorflow DNN on MNIST')\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                           name='tf-mnist-svc', \n",
    "                           models=[model], \n",
    "                           inference_config=inference_config, \n",
    "                           deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scoring web service endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed model\n",
    "Let's test the deployed model. Pick 30 random samples from the test set, and send it to the web service hosted in ACI. Note here we are using the `run` API in the SDK to invoke the service. You can also make raw HTTP calls using any HTTP tool such as curl.\n",
    "\n",
    "After the invocation, we print the returned predictions and plot them along with the input images. Use red font color and inversed image (white on black) to highlight the misclassified samples. Note since the model accuracy is pretty high, you might have to run the below cell a few times before you can see a misclassified sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# find 30 random samples from test set\n",
    "n = 30\n",
    "sample_indices = np.random.permutation(X_test.shape[0])[0:n]\n",
    "\n",
    "test_samples = json.dumps({\"data\": X_test[sample_indices].tolist()})\n",
    "test_samples = bytes(test_samples, encoding='utf8')\n",
    "\n",
    "# predict using the deployed model\n",
    "result = service.run(input_data=test_samples)\n",
    "\n",
    "# compare actual value vs. the predicted values:\n",
    "i = 0\n",
    "plt.figure(figsize = (20, 1))\n",
    "\n",
    "for s in sample_indices:\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    \n",
    "    # use different color for misclassified sample\n",
    "    font_color = 'red' if y_test[s] != result[i] else 'black'\n",
    "    clr_map = plt.cm.gray if y_test[s] != result[i] else plt.cm.Greys\n",
    "    \n",
    "    plt.text(x=10, y=-10, s=y_hat[s], fontsize=18, color=font_color)\n",
    "    plt.imshow(X_test[s].reshape(28, 28), cmap=clr_map)\n",
    "    \n",
    "    i = i + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also send raw HTTP request to the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(X_test)-1)\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(X_test[random_index])) + \"]}\"\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "#print(\"input data:\", input_data)\n",
    "print(\"label:\", y_test[random_index])\n",
    "print(\"prediction:\", resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the workspace after the web service was deployed. You should see \n",
    "* a registered model named 'model' and with the id 'model:1'\n",
    "* a webservice called 'tf-mnist' with some scoring URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ws.models['tf-dnn-mnist']\n",
    "print(\"Model: {}, ID: {}\".format('tf-dnn-mnist', model.id))\n",
    "    \n",
    "webservice = ws.webservices['tf-mnist-svc']\n",
    "print(\"Webservice: {}, scoring URI: {}\".format('tf-mnist-svc', webservice.scoring_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "You can delete the ACI deployment with a simple delete API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "swatig"
   }
  ],
  "category": "training",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "MNIST"
  ],
  "deployment": [
   "Azure Container Instance"
  ],
  "exclude_from_index": false,
  "framework": [
   "TensorFlow"
  ],
  "friendly_name": "Training and hyperparameter tuning using the TensorFlow estimator",
  "index_order": 1,
  "kernelspec": {
   "display_name": "azure",
   "language": "python",
   "name": "azure"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "tags": [
   "None"
  ],
  "task": "Train a deep neural network"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
